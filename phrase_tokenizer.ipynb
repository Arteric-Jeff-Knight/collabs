{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "phrase_tokenizer.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Arteric-Jeff-Knight/collabs/blob/master/phrase_tokenizer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sAYGFJ-5J4M9"
      },
      "source": [
        "## Run the first code block once to load libraries and define functions.  \n",
        "\n",
        "Ignore any output that isn't an error. Process takes a minute or so, but is finished when you see:\n",
        "\n",
        "<font color=\"green\">âœ” Download and installation successful</font><br>\n",
        "You can now load the model via spacy.load('en_core_web_sm')<br>\n",
        "This script is now done."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "28poeoUG0seE",
        "cellView": "form"
      },
      "source": [
        "#@title <= Run first one time\n",
        "\n",
        "!pip install spacy==3.0.5 textacy==0.11.0 phrasemachine  # --upgrade \n",
        "\n",
        "import IPython\n",
        "import uuid\n",
        "from google.colab import output\n",
        "from google.colab import files\n",
        "import ipywidgets as widgets\n",
        "import io, re, string, unicodedata\n",
        "from itertools import dropwhile\n",
        "import numpy as np\n",
        "import pandas as pd \n",
        "from collections import Counter\n",
        "import spacy\n",
        "import textacy\n",
        "from textacy import extract\n",
        "import phrasemachine\n",
        "\n",
        "compiled_word_counter = re.compile(r'\\w+')\n",
        "\n",
        "# print(spacy.__version__)\n",
        "\n",
        "# functions used several times\n",
        "def drop_fewer_than(counter_dict: Counter, threshold: int = 1):\n",
        "  for key, count in dropwhile(lambda key_count: key_count[1] >= threshold, counter_dict.most_common()):\n",
        "      del counter_dict[key]\n",
        "\n",
        "def split_df_into_data_and_configs(uploaded, defaults: dict = {}, config_name: str = 'config'):\n",
        "  # Put uploaded file into dataframe\n",
        "  filename = list(uploaded.keys())[0]\n",
        "  df = pd.read_csv(io.BytesIO(uploaded[filename]),header=None)\n",
        "\n",
        "  # Get everything with 'configs' in first coumn\n",
        "  configs = df[df[0] == config_name] \n",
        "  # Build a dictionary from the key in the second column with values from the third\n",
        "  defaults.update(dict(zip(configs[1], configs[2])))\n",
        "\n",
        "  # Everything else that isn't a config, is data\n",
        "  data = df[df[0] != config_name].reset_index(drop=True)\n",
        "  # Assume that the first row is the column names now that configs are gone\n",
        "  data.columns = data.iloc[0]\n",
        "  # Drop the row with the column names\n",
        "  data.drop(df.index[0], inplace=True)\n",
        "  # Reset the index, so zero works below\n",
        "  data = data.reset_index(drop=True)\n",
        "\n",
        "  # Validate the column name configs\n",
        "  defaults['columns'] = list(data.columns)\n",
        "\n",
        "  if 'tokenize_col_in' not in defaults:\n",
        "      defaults['tokenize_col_in'] = defaults['columns'][0]\n",
        "  elif defaults['tokenize_col_in'] not in defaults['columns']:\n",
        "    # With nothing defined or garbage, use first column\n",
        "    if defaults['tokenize_col_in'].capitalize() in defaults['columns']:\n",
        "      defaults['tokenize_col_in'] = defaults['tokenize_col_in'].capitalize()\n",
        "    else:\n",
        "      defaults['tokenize_col_in'] = defaults['columns'][0]\n",
        "\n",
        "  if 'tokenize_file_out' not in defaults:\n",
        "    defaults['tokenize_file_out'] = '-tokenized'\n",
        "\n",
        "  if 'tokenize_output_filename' not in defaults:\n",
        "      defaults['tokenize_output_filename'] = filename.replace('.csv', f\"{defaults['tokenize_file_out']}.csv\")\n",
        "\n",
        "  try:\n",
        "    defaults['tokenize_ng_threshold'] = min(6,int(defaults['tokenize_ng_threshold']))\n",
        "    if defaults['tokenize_ng_threshold'] < 3:\n",
        "      defaults['tokenize_ng_threshold'] = tuple([2])\n",
        "    else:\n",
        "      defaults['tokenize_ng_threshold'] = tuple(range(2,defaults['tokenize_ng_threshold']+1))\n",
        "  except Exception as e:\n",
        "    defaults['tokenize_ng_threshold'] = (2,3,4)\n",
        "\n",
        "  try:\n",
        "    defaults['tokenize_min_token_count'] = min(9,int(defaults['tokenize_min_token_count']))\n",
        "    if defaults['tokenize_min_token_count'] < 1:\n",
        "      defaults['tokenize_min_token_count'] = 1\n",
        "  except Exception as e:\n",
        "    defaults['tokenize_min_token_count'] = 3\n",
        "\n",
        "  return data, defaults\n",
        "\n",
        "spacy.cli.download('en_core_web_sm')\n",
        "nlp = textacy.load_spacy_lang('en_core_web_sm')\n",
        "\n",
        "print('This script is now done.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ek12BXAFKnLA"
      },
      "source": [
        "# Next, run the following code block to upload a file for processing and set options\n",
        "\n",
        "If you want to process multiple files, start here for each one, no need to run the first block again and again.\n",
        "\n",
        "Processing is mostly handled by default values, but if you need to override them, configuration is handled by passing values in the input file. See **How to Configure** below for instructions.\n",
        "\n",
        "The code will ask you to choose a file to import and select which column to tokenize and the maximum number of words in an n-gram. After uploading a file, you can change these values to influence the next steps. You do not need to re-run this script to change the option values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sfU8YqyfVv-l",
        "cellView": "form"
      },
      "source": [
        "#@title <= Run once for every file to tokenize\n",
        "\n",
        "defaults = {\n",
        "  'tokenize_col_in': 'lemmatized',\n",
        "  'tokenize_file_out': '-tokenized',\n",
        "  'tokenize_ng_threshold': 4,\n",
        "  'tokenize_min_phrase_count': 3\n",
        "}\n",
        "\n",
        "df, configs = split_df_into_data_and_configs(files.upload(), defaults)\n",
        "\n",
        "col_in = widgets.Dropdown(options=configs['columns'], value=configs['tokenize_col_in'])\n",
        "\n",
        "print('\\nSelect the column that contains the text to be tokenized:')\n",
        "display(col_in)\n",
        "\n",
        "ng_threshold = widgets.Dropdown(options=[('No n-grams', 0), \n",
        "  ('2-grams', (2, )), \n",
        "  ('3-grams', (2, 3)), \n",
        "  ('4-grams', (2, 3, 4)), \n",
        "  ('5-grams', (2, 3, 4, 5)), \n",
        "  ('6-grams', (2, 3, 4, 5, 6))], value=configs['tokenize_ng_threshold'])\n",
        "print('\\nWhat is the maximum size of the n-grams (zero to not run n-grams):')\n",
        "display(ng_threshold)\n",
        "\n",
        "print('\\nConfigs:')\n",
        "for key in configs:\n",
        "  if 'tokenize_' in key:\n",
        "    print('   ',key,':',configs[key])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FGE57rCpLzEf"
      },
      "source": [
        "# This code block tokenizes the data in the upload file\n",
        "\n",
        "If you want to process multiple files, start here for each one, no need to run the first block again and again.\n",
        "\n",
        "Processing is mostly handled by default values, but if you need to override them, configuration is handled by passing values in the input file. See **How to Configure** below for instructions.\n",
        "\n",
        "The code will ask you to choose a file to import and select which column to tokenize and the maximum number of words in an n-gram. After uploading a file, you can change these values to influence the next steps. You do not need to re-run this script to change the option values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DLRLWT12We8g",
        "cellView": "form"
      },
      "source": [
        "#@title <= Run every time you change the column and n-gram settings above\n",
        "text_df = df[[col_in.value]]\n",
        "\n",
        "pm_phrases = []\n",
        "cluster_phrases = []\n",
        "textrank_phrases = []\n",
        "yake_phrases = []\n",
        "scake_phrases = []\n",
        "sgrank_phrases = []\n",
        "ngram_phrases = []\n",
        "\n",
        "\n",
        "for docrow in text_df[col_in.value].squeeze():\n",
        "  docrow = str(docrow)\n",
        "  doc = textacy.make_spacy_doc(docrow, lang='en_core_web_sm')\n",
        "\n",
        "  word_count = len(doc)\n",
        "\n",
        "  # Start with the phrasemachine, maybe we can bail on the others\n",
        "  pm = phrasemachine.get_phrases(docrow)\n",
        "  for phrase in pm['counts']:\n",
        "    pm_phrases.append({'token': phrase, 'phrasemachine_count': pm['counts'][phrase], 'phrasemachine_docs': 1})\n",
        "      \n",
        "  # if nc_count.value:\n",
        "  chunks = Counter()\n",
        "  for nc in doc.noun_chunks:\n",
        "    # add only if is a phrase and does not start with a stop word\n",
        "    if \" \" in nc.text and nc.text.split()[0] not in spacy.lang.en.stop_words.STOP_WORDS:\n",
        "      chunks[nc.text] += 1\n",
        "\n",
        "  # a round about way to preserve token frequency and document frequency\n",
        "  for token in chunks:\n",
        "    cluster_phrases.append({'token': token, 'cluster_count': chunks[token], 'cluster_docs': 1})\n",
        "\n",
        "  try:\n",
        "    for token, rank in extract.keyterms.textrank(doc=doc):\n",
        "      if \" \" in token: # only add tokens that are phrases!\n",
        "        textrank_phrases.append({'token': token, 'tr_count': 1, 'tr_docs': 1, 'textrank': rank})\n",
        "  except Exception as e:\n",
        "    pass\n",
        "\n",
        "  try:\n",
        "    for token, rank in extract.keyterms.yake(doc=doc):\n",
        "      if \" \" in token: # only add tokens that are phrases!\n",
        "        yake_phrases.append({'token': token, 'yake_count': 1, 'yake_docs': 1, 'yake': rank})\n",
        "  except Exception as e:\n",
        "    pass\n",
        "\n",
        "  try:\n",
        "    for token, rank in extract.keyterms.scake(doc=doc):\n",
        "      if \" \" in token: # only add tokens that are phrases!\n",
        "        scake_phrases.append({'token': token, 'scake_count': 1, 'scake_docs': 1, 'scake': rank})\n",
        "  except Exception as e:\n",
        "    pass\n",
        "\n",
        "  try:\n",
        "    for token, rank in extract.keyterms.sgrank(doc=doc):\n",
        "      if \" \" in token: # only add tokens that are phrases!\n",
        "        sgrank_phrases.append({'token': token, 'sgrank_count': 1, 'sgrank_docs': 1, 'sgrank': rank})\n",
        "  except Exception as e:\n",
        "    pass\n",
        "\n",
        "  bot = doc._.to_bag_of_terms(ngs=ng_threshold.value, ents=True, weighting=\"count\") #, as_strings=True)\n",
        "  for term in bot:\n",
        "    if \" \" in term:  # for some reason the function returns 1-grams, even if not asked to\n",
        "        ngram_phrases.append({'token': term, 'ng_count': bot[term], 'ng_docs': 1, 'ng_freq': bot[term]/word_count})\n",
        "\n",
        "# Drop all rows with count below threshold\n",
        "# ncdf = ncdf[ncdf.noun_cluster_count > nc_count.value]\n",
        "\n",
        "if pm_phrases:\n",
        "  pmdf = pd.DataFrame(pm_phrases)\n",
        "  # Group by token\n",
        "  pmdf = pmdf.groupby('token', as_index=False).agg({'phrasemachine_count': 'sum', 'phrasemachine_docs': 'sum'})  \n",
        "else:\n",
        "  # the merges later on will require an empty dataframe\n",
        "  pmdf = pd.DataFrame([], columns=['token', 'phrasemachine_count', 'phrasemachine_docs'])\n",
        "\n",
        "if cluster_phrases:\n",
        "  ncdf = pd.DataFrame(cluster_phrases)\n",
        "  # Group by token\n",
        "  ncdf = ncdf.groupby('token', as_index=False).agg({'cluster_count': 'sum', 'cluster_docs': 'sum'}) \n",
        "else:\n",
        "  # the merges later on will require an empty dataframe\n",
        "  ncdf = pd.DataFrame([], columns=['token', 'cluster_count', 'cluster_docs'])\n",
        "\n",
        "if textrank_phrases:\n",
        "  trdf = pd.DataFrame(textrank_phrases)\n",
        "  # Group by token\n",
        "  trdf = trdf.groupby('token', as_index=False).agg({'tr_count': 'sum', 'tr_docs': 'sum', 'textrank': 'max'})   \n",
        "else:\n",
        "  # the merges later on will require an empty dataframe\n",
        "  trdf = pd.DataFrame([], columns=['token', 'tr_count', 'tr_docs'])\n",
        "\n",
        "if yake_phrases:\n",
        "  ykdf = pd.DataFrame(yake_phrases)\n",
        "  # Group by token\n",
        "  ykdf = ykdf.groupby('token', as_index=False).agg({'yake_count': 'sum', 'yake_docs': 'sum', 'yake': 'max'}) \n",
        "else:\n",
        "  # the merges later on will require an empty dataframe\n",
        "  ykdf = pd.DataFrame([], columns=['token', 'yake_count', 'yake_docs'])\n",
        "\n",
        "if scake_phrases:\n",
        "  scdf = pd.DataFrame(scake_phrases)\n",
        "  # Group by token\n",
        "  scdf = scdf.groupby('token', as_index=False).agg({'scake_count': 'sum', 'scake_docs': 'sum', 'scake': 'max'}) \n",
        "else:\n",
        "  # the merges later on will require an empty dataframe\n",
        "  scdf = pd.DataFrame([], columns=['token', 'scake_count', 'scake_docs'])\n",
        "\n",
        "if sgrank_phrases:\n",
        "  sgdf = pd.DataFrame(sgrank_phrases)\n",
        "  # Group by token\n",
        "  sgdf = sgdf.groupby('token', as_index=False).agg({'sgrank_count': 'sum', 'sgrank_docs': 'sum', 'sgrank': 'max'}) \n",
        "else:\n",
        "  # the merges later on will require an empty dataframe\n",
        "  sgdf = pd.DataFrame([], columns=['token', 'sgrank_count', 'sgrank_docs'])\n",
        "\n",
        "if ngram_phrases:\n",
        "  ngdf = pd.DataFrame(ngram_phrases)\n",
        "  # Group by token\n",
        "  ngdf = ngdf.groupby('token', as_index=False).agg({'ng_count': 'sum', 'ng_docs': 'sum'}) \n",
        "else:\n",
        "  ngdf = pd.DataFrame([], columns=['token', 'ng_count', 'ng_docs'])\n",
        "\n",
        "merged = pd.merge(left=pmdf, right=ncdf, left_on='token', right_on='token',how='outer')\n",
        "merged = pd.merge(left=merged, right=trdf, left_on='token', right_on='token',how='outer')\n",
        "merged = pd.merge(left=merged, right=ykdf, left_on='token', right_on='token',how='outer')\n",
        "merged = pd.merge(left=merged, right=scdf, left_on='token', right_on='token',how='outer')\n",
        "merged = pd.merge(left=merged, right=ngdf, left_on='token', right_on='token',how='outer')\n",
        "merged = merged.fillna(0)\n",
        "cols = ['phrasemachine_count', 'phrasemachine_docs', \n",
        "        'cluster_count', 'cluster_docs',\n",
        "        'tr_count', 'tr_docs',\n",
        "        'yake_count', 'yake_docs',\n",
        "        'scake_count', 'scake_docs',\n",
        "        'ng_count', 'ng_docs' ]\n",
        "merged[cols] = merged[cols].astype(int)\n",
        "\n",
        "merged['words'] = merged.apply(lambda x: len(compiled_word_counter.findall(x['token'])), axis=1)\n",
        "\n",
        "# Add word count\n",
        "def reduce_merged_to_count(mdf, min_count: int = 3):\n",
        "  return mdf[\n",
        "    (mdf['phrasemachine_count'] >= min_count) \n",
        "    | (mdf['cluster_count'] >= min_count)\n",
        "    | (mdf['tr_count'] >= min_count)\n",
        "    | (mdf['yake_count'] >= min_count)\n",
        "    | (mdf['scake_count'] >= min_count)\n",
        "    | (mdf['ng_count'] >= min_count)\n",
        "    ]\n",
        "\n",
        "rowcount = []\n",
        "for count in range(1,10):\n",
        "  rows = len(reduce_merged_to_count(merged, count))\n",
        "  rowcount.append((f'{count} or more for {rows} phrases',count))\n",
        "\n",
        "min_token_count = widgets.Dropdown(options=rowcount,value=configs['tokenize_min_token_count'])\n",
        "print('\\nHow many times must a phrase, noun cluster or n-gram appear to be counted:')\n",
        "display(min_token_count)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ljOoRFq_SUbu"
      },
      "source": [
        "# Finally, set the phrase count option above and download\n",
        "\n",
        "You can run both scripts once, and then change the option to generate different downloads.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "05OsJTOShx7q",
        "cellView": "form"
      },
      "source": [
        "#@title <= Run this code to get download button.\n",
        "\n",
        "class InvokeButton(object):\n",
        "  def __init__(self, title, callback):\n",
        "    self._title = title\n",
        "    self._callback = callback\n",
        "\n",
        "  def _repr_html_(self):\n",
        "    callback_id = 'button-' + str(uuid.uuid4())\n",
        "    output.register_callback(callback_id, self._callback)\n",
        "\n",
        "    template = \"\"\"<button id=\"{callback_id}\">{title}</button>\n",
        "        <script>\n",
        "          document.querySelector(\"#{callback_id}\").onclick = (e) => {{\n",
        "            google.colab.kernel.invokeFunction('{callback_id}', [], {{}})\n",
        "            e.preventDefault();\n",
        "          }};\n",
        "        </script>\"\"\"\n",
        "    html = template.format(title=self._title, callback_id=callback_id)\n",
        "    return html\n",
        "\n",
        "def download_tokens():\n",
        "  try:\n",
        "    reduce_merged_to_count(merged, min_token_count.value).to_csv(configs['tokenize_output_filename'], index=False)\n",
        "    files.download(configs['tokenize_output_filename'])\n",
        "  except Exception as e:\n",
        "    print('ex',e)\n",
        "\n",
        "InvokeButton('Download Tokens', download_tokens)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4LrljB3OkP44"
      },
      "source": [
        "# Returned File\n",
        "\n",
        "The columns in the generated file are:\n",
        "\n",
        "|  column | explanation |\n",
        "|----|:----|\n",
        "|`token` | The token identified by one or more of the processes |\n",
        "|`phrasemachine_count` | Number of times the token is identified as phrase by the *phrasemachine* algorithm |\n",
        "|`phrasemachine_docs` | Number of rows the token is identified by the *phrasemachine* algorithm |\n",
        "|`cluster_count` | Number of times the token is identified by spaCy as a *Noun Cluster* in all rows (must exceed `tokenize_cluster_min` to be counted at all) |\n",
        "|`cluster_docs` | Number of rows the token is identified by spaCy as a *Noun Cluster* |\n",
        "|`tr_count` | Number of times the token is identified as phrase by the *TextRank* algorithm in all rows (must exceed `tokenize_textrank_min` to be counted at all) |\n",
        "|`tr_docs` | Number of rows the token is identified by the the *TextRank* algorithm |\n",
        "|`textrank` | The maximum *TextRank* value for the token across all documents |\n",
        "|`yake_count` | Number of times the token is identified as phrase by the *YAKE* algorithm in all rows (must exceed `tokenize_yake_min` to be counted at all) |\n",
        "|`yake_docs` | Number of rows the token is identified by the the *YAKE* algorithm |\n",
        "|`yake` | The maximum *YAKE* score for the token across all documents |\n",
        "|`scake_count` | Number of times the token is identified as phrase by the *sCAKE* algorithm in all rows (must exceed `tokenize_scake_min` to be counted at all) |\n",
        "|`scake_docs` | Number of rows the token is identified by the the *sCAKE* algorithm |\n",
        "|`scake` | The maximum *sCAKE* score for the token across all documents |\n",
        "|`sgrank_count` | Number of times the token is identified as phrase by the *SGRank* algorithm in all rows (must exceed `tokenize_sgrank_min` to be counted at all) |\n",
        "|`sgrank_docs` | Number of rows the token is identified by the the *SGRank* algorithm |\n",
        "|`sgrank` | The maximum *SGRank* score for the token across all documents |\n",
        "|`ngram_count` | Number of times the token is identified as an *n*-gram in all rows where *n* is defined by `tokenize_ngram_limit` (must exceed `tokenize_ngram_min` to be counted at all)  |\n",
        "|`ngram_docs` | Number of rows the token is identified as an *n*-gram |\n",
        "|`words` | The token's word count |\n",
        "# **Algorithms**\n",
        "\n",
        "The processor uses several methods to identify phrases and key tokens from the input file. \n",
        "\n",
        "## **phrasemachine**:  \n",
        "\n",
        "An implelmentation of the algorithm found in the paper [Bag of What? Simple Noun Phrase Extraction for Text Analysis](http://brenocon.com/handler2016phrases.pdf). A new phrase-based method, **NPFST**, for enriching a unigram BOW. NPFST uses a part-of-speech tagger and a finite state transducer to extract multiword phrases to be added to a unigram BOW.\n",
        "\n",
        "## **TextRank**:  \n",
        "\n",
        "https://www.aclweb.org/anthology/W04-3252.pdf\n",
        "\n",
        "In this paper, we introduce TextRank â€“ a graph-based\n",
        "ranking model for text processing, and show how this\n",
        "model can be successfully used in natural language\n",
        "applications. In particular, we propose two innovative unsupervised methods for keyword and sentence\n",
        "extraction, and show that the results obtained compare favorably with previously published results on\n",
        "established benchmarks.\n",
        "\n",
        "## **YAKE**:  A Text Feature Based Automatic Keyword Extraction Method for Single Documents\n",
        "\n",
        "https://repositorio.inesctec.pt/bitstream/123456789/7622/1/P-00N-NF3.pdf\n",
        "\n",
        "A lightweight approach for keyword\n",
        "extraction and ranking based on an unsupervised methodology to select the most\n",
        "important keywords of a single document. To understand the merits of our\n",
        "proposal, we compare it against RAKE, TextRank and SingleRank methods\n",
        "(three well-known unsupervised approaches) and the baseline TF.IDF, over four\n",
        "different collections to illustrate the generality of our approach. The experimental results suggest that extracting keywords from documents using our\n",
        "method results in a superior effectiveness when compared to similar approaches.\n",
        "\n",
        "## **sCAKE**: Semantic Connectivity Aware Keyword Extraction\n",
        "\n",
        "https://arxiv.org/abs/1811.10831v1\n",
        "\n",
        "Keyword Extraction is an important task in several text analysis endeavors. In this paper, we present a critical discussion of the issues and challenges ingraph-based keyword extraction methods, along with comprehensive empirical analysis. We propose a parameterless method for constructing graph of text that captures the contextual relation between words. A novel word scoring method is also proposed based on the connection between concepts. We demonstrate that both proposals are individually superior to those followed by the state-of-the-art graph-based keyword extraction algorithms. Combination of the proposed graph construction and scoring methods leads to a novel, parameterless keyword extraction method (sCAKE) based on semantic connectivity of words in the document.\n",
        "\n",
        "Motivated by limited availability of NLP tools for several languages, we also design and present a language-agnostic keyword extraction (LAKE) method. We eliminate the need of NLP tools by using a statistical filter to identify candidate keywords before constructing the graph. We show that the resulting method is a competent solution for extracting keywords from documents oflanguages lacking sophisticated NLP support.\n",
        "\n",
        "## **SGRank**: Combining Statistical and Graphical Methods to Improve the State of the Art in Unsupervised Keyphrase Extraction\n",
        "\n",
        "https://www.aclweb.org/anthology/S15-1013.pdf\n",
        "\n",
        "Keyphrase extraction is a fundamental technique in natural language processing. It enables documents to be mapped to a concise set of phrases that can be used for indexing, clustering, ontology building, auto-tagging and other information organization schemes. Two major families of unsupervised keyphrase extraction algorithms may be characterized as statistical and graph-based. We present a hybrid statistical-graphical algorithm that capitalizes on the heuristics of both families of algorithms and is able to outperform the state of the art in unsupervised keyphrase extraction on several datasets.\n",
        "\n",
        "## ***n*-grams**: \n",
        "\n",
        "The simplest algorithm at all. For integer values of *n* from 2 to 6, ...\n",
        "\n",
        "# **How to Configure**\n",
        "\n",
        "To add configurations to a file, put the value `config` in the first column (no matter what the header) and the configuration key name in the second, and the value to be set in the third (anything beyond that can be ignored). Durding processing these rows will be separated from the data and not included in the returned file. For convenience, they can appear anywhere in the incoming file: before the headers, at the end, anywhere in between or even mixed among the data.\n",
        "\n",
        "For this particular operation, the configurations just set the default values for the option dropdowns, and are inlcuded mainly for consistency and a future option to operate without interaction.\n",
        "\n",
        "The possible configuration keys and their default values are:\n",
        "\n",
        "| Key | Defualt | Notes |\n",
        "|--------------|:-----------|:------|\n",
        "| `tokenize_col_in` | lemmatized | *The exact, case sensitive name of column in the incoming file to process* |\n",
        "| `tokenize_file_out` | -tokenized | *The text to add to the filename that is returned* |\n",
        "| `tokenize_output_filename` | | *If passed, this value overrides the value calulated by applyting `tokenize_file_out` to the uploaded filename* |\n",
        "| `tokenize_ng_threshold` | `4` | *The maximum word length of the generated n-grams* |\n",
        "| `tokenize_min_token_count` | `3` | *The minimum times a phrase must occur in all documents to be counted* |\n",
        "\n",
        "- If the value in `tokenize_col_in` does not match any header in the uploaded file, the content in the first column will be processed.\n",
        "- The value in `tokenize_file_out` will be inserted between the base filename and the '.csv' of the uploaded file to create `tokenize_output_filename` unless a value is passed as a configuration.\n",
        "\n",
        "# **Sample Configuration to Copy and Paste**\n",
        "\n",
        "No need to add them all to a file, just the ones you want to change:\n",
        "\n",
        "```\n",
        "tokenize_col_in,lemmatized\n",
        "tokenize_file_out,-tokenized\n",
        "tokenize_output_filename,custom_filename.csv\n",
        "tokenize_ng_threshold\n",
        "tokenize_min_token_count\n",
        "```\n"
      ]
    }
  ]
}