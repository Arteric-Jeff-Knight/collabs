{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tokenizer.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Arteric-Jeff-Knight/collabs/blob/master/tokenizer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S_AezM4YZE1s",
        "cellView": "code"
      },
      "source": [
        "from google.colab import files\n",
        "import ipywidgets as widgets\n",
        "import io, re, string, unicodedata                          # Import Regex, string and unicodedata.\n",
        "from itertools import dropwhile\n",
        "import numpy as np\n",
        "import pandas as pd                                     # Import pandas.\n",
        "from collections import Counter\n",
        "!pip install spacy==2.3.5  # need 2.3.5 for textacy\n",
        "import spacy\n",
        "!pip install textacy\n",
        "import textacy\n",
        "import textacy.ke.textrank\n",
        "\n",
        "# functions used several times\n",
        "def drop_fewer_than(counter_dict: Counter, threshold: int = 1):\n",
        "  for key, count in dropwhile(lambda key_count: key_count[1] >= threshold, counter_dict.most_common()):\n",
        "      del counter_dict[key]\n",
        "\n",
        "#print(spacy.__version__)\n",
        "count_words = re.compile(r'\\w+')\n",
        "\n",
        "\"\"\"\n",
        "print('\\n\\nSelect the spaCy language model:')\n",
        "ddopts = [('Fast - No vectors, 13 MB : en_core_web_sm', 'en_core_web_sm'), \n",
        "\t('685k keys, 20k unique vectors, 44 MB : en_core_web_md', 'en_core_web_md'), \n",
        "\t('Slow - 685k keys, 685k unique vectors, 742 MB : en_core_web_lg', 'en_core_web_lg')]\n",
        "\n",
        "load_lib = widgets.Dropdown(options=ddopts)\n",
        "display(load_lib)\n",
        "\"\"\"\n",
        "\n",
        "\"\"\"\n",
        "print('\\nHow many times must a Noun Cluster appear to be counted (zero to not run Noun Clusters):')\n",
        "nc_count = widgets.Dropdown(options=range(10), value=3)\n",
        "display(nc_count)\n",
        "\"\"\"\n",
        "\"\"\"\n",
        "print('\\nHow many times must a TextRank Phrase appear to be counted (zero to not run TextRank):')\n",
        "tr_count = widgets.Dropdown(options=range(10), value=2)\n",
        "display(tr_count)\n",
        "\"\"\"\n",
        "\n",
        "ng_threshold = widgets.Dropdown(options=[('No n-grams', 0), \n",
        "  ('2-grams', (1, 2)), \n",
        "  ('3-grams', (1, 2, 3)), \n",
        "  ('4-grams', (1, 2, 3, 4)), \n",
        "  ('5-grams', (1, 2, 3, 4, 5)), \n",
        "  ('6-grams', (1, 2, 3, 4, 5, 6))], value=(1, 2, 3, 4))\n",
        "ng_count = widgets.Dropdown(options=range(1,10), value=3)\n",
        "print('\\nWhat is the maximum size of the n-grams (zero to not run n-grams):')\n",
        "display(ng_threshold)\n",
        "print('How many times must the n-gram appear to be counted:')\n",
        "display(ng_count)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sfU8YqyfVv-l"
      },
      "source": [
        "spacy.cli.download('en_core_web_sm')\n",
        "nlp = textacy.load_spacy_lang('en_core_web_sm')\n",
        "uploaded = files.upload()\n",
        "filename = list(uploaded.keys())[0]\n",
        "print('uploded file: ',filename)\n",
        "text_df = pd.read_csv(io.BytesIO(uploaded[filename]))\n",
        "text_df.head(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-1aHWIzNWe5p",
        "cellView": "form"
      },
      "source": [
        "#@title\n",
        "col_list = list(text_df.columns)\n",
        "if 'lemmatized' in col_list:\n",
        "  default_col = 'lemmatized'\n",
        "elif 'clean_text' in col_list:\n",
        "  default_col = 'clean_text'\n",
        "else:\n",
        "  default_col = None\n",
        "\n",
        "col_in = widgets.Dropdown(options=col_list, value=default_col)\n",
        "\n",
        "print('\\nSelect the column that contains the text to be tokenized:')\n",
        "display(col_in)\n",
        "\n",
        "print('\\nName the file to return:')\n",
        "output_filename = widgets.Text(value=filename.replace('.csv','-tokenized.csv') )\n",
        "display(output_filename)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DLRLWT12We8g",
        "cellView": "code"
      },
      "source": [
        "text_df = text_df[[col_in.value]]\n",
        "\n",
        "cluster_phrases = []\n",
        "textrank_phrases = []\n",
        "yake_phrases = []\n",
        "scake_phrases = []\n",
        "sgrank_phrases = []\n",
        "ngram_phrases = []\n",
        "\n",
        "for docrow in text_df[col_in.value].squeeze():\n",
        "  doc = textacy.make_spacy_doc(str(docrow), lang='en')\n",
        "  word_count = len(doc)\n",
        "  # if nc_count.value:\n",
        "  chunks = Counter()\n",
        "  for nc in doc.noun_chunks:\n",
        "    if len(nc) > 1:\n",
        "      chunks[nc.text] += 1\n",
        "  # a round about way to preserve token frequency and document frequency\n",
        "  for token in chunks:\n",
        "    cluster_phrases.append({'token': token, 'noun_cluster_frequency': chunks[token], 'noun_cluster_documents': 1})\n",
        "\n",
        "  textranks = textacy.ke.textrank(doc=doc)\n",
        "  for token, rank in textranks:\n",
        "    textrank_phrases.append({'token': token, 'textrank_frequency': 1, 'textrank_documents': 1, 'textrank': rank})\n",
        "\n",
        "  try:\n",
        "    for token, rank in textacy.ke.textrank(doc=doc):\n",
        "      textrank_phrases.append({'token': token, 'textrank_frequency': 1, 'textrank_documents': 1, 'textrank': rank})\n",
        "  except Exception as e:\n",
        "    pass\n",
        "\n",
        "  try:\n",
        "    for token, rank in textacy.ke.yake(doc=doc):\n",
        "      yake_phrases.append({'token': token, 'yake_frequency': 1, 'yake_documents': 1, 'yake': rank})\n",
        "  except Exception as e:\n",
        "    pass\n",
        "\n",
        "  try:\n",
        "    for token, rank in textacy.ke.scake(doc=doc):\n",
        "      scake_phrases.append({'token': token, 'scake_frequency': 1, 'scake_documents': 1, 'scake': rank})\n",
        "  except Exception as e:\n",
        "    pass\n",
        "\n",
        "  try:\n",
        "    for token, rank in textacy.ke.sgrank(doc=doc):\n",
        "      sgrank_phrases.append({'token': token, 'sgrank_frequency': 1, 'sgrank_documents': 1, 'sgrank': rank})\n",
        "  except Exception as e:\n",
        "    pass\n",
        "\n",
        "  if ng_threshold.value:\n",
        "    bot = doc._.to_bag_of_terms(ngrams=ng_threshold.value, entities=True, weighting=\"count\", as_strings=True)\n",
        "    for term in bot:\n",
        "      if len(term) > 1:\n",
        "        ngram_phrases.append({'token': term, 'ngram_frequency': bot[term], 'ngram_documents': 1, 'tf': bot[term]/word_count})\n",
        "\n",
        "if cluster_phrases:\n",
        "  ncdf = pd.DataFrame(cluster_phrases)\n",
        "  # Group by token\n",
        "  ncdf = ncdf.groupby('token', as_index=False).agg({'noun_cluster_frequency': 'sum', 'noun_cluster_documents': 'sum'}) \n",
        "  # Drop all rows with count below threshold\n",
        "  # ncdf = ncdf[ncdf.noun_cluster_frequency > nc_count.value]\n",
        "else:\n",
        "  # the merges later on will require an empty dataframe\n",
        "  ncdf = pd.DataFrame([], columns=['token', 'noun_cluster_frequency', 'noun_cluster_documents'])\n",
        "\n",
        "if textrank_phrases:\n",
        "  # Build the DataFrame\n",
        "  trdf = pd.DataFrame(textrank_phrases)\n",
        "  ykdf = pd.DataFrame(yake_phrases)\n",
        "  scdf = pd.DataFrame(scake_phrases)\n",
        "  sgdf = pd.DataFrame(sgrank_phrases)\n",
        "  # Group by token\n",
        "  trdf = trdf.groupby('token', as_index=False).agg({'textrank_frequency': 'sum', 'textrank_documents': 'sum', 'textrank': 'max'}) \n",
        "  ykdf = ykdf.groupby('token', as_index=False).agg({'yake_frequency': 'sum', 'yake_documents': 'sum', 'yake': 'max'}) \n",
        "  scdf = scdf.groupby('token', as_index=False).agg({'scake_frequency': 'sum', 'scake_documents': 'sum', 'scake': 'max'}) \n",
        "  sgdf = sgdf.groupby('token', as_index=False).agg({'sgrank_frequency': 'sum', 'sgrank_documents': 'sum', 'sgrank': 'max'}) \n",
        "  m1 = pd.merge(left=trdf, right=ykdf, left_on='token', right_on='token',how='outer')\n",
        "  m2 = pd.merge(left=scdf, right=sgdf, left_on='token', right_on='token',how='outer')\n",
        "  trm = pd.merge(left=m1, right=m2, left_on='token', right_on='token',how='outer')\n",
        "  # Drop all rows with count below threshold\n",
        "  # trdf = trdf[trdf.textrank_frequency > tr_count.value]\n",
        "else:\n",
        "  # the merges later on will require an empty dataframe\n",
        "  trm = pd.DataFrame([], columns=['token'])\n",
        "\n",
        "\n",
        "if ngram_phrases:\n",
        "  ngdf = pd.DataFrame(ngram_phrases)\n",
        "  # Group by token\n",
        "  ngdf = ngdf.groupby('token', as_index=False).agg({'ngram_frequency': 'sum', 'ngram_documents': 'sum'}) \n",
        "  # Drop all rows with count below threshold\n",
        "  ngdf = ngdf[ngdf.ngram_frequency > ng_count.value]\n",
        "else:\n",
        "  ngdf = pd.DataFrame([], columns=['token', 'ngram_frequency', 'ngram_documents'])\n",
        "\n",
        "merged = pd.merge(left=ncdf, right=ngdf, left_on='token', right_on='token',how='outer')\n",
        "merged = pd.merge(left=merged, right=trm, left_on='token', right_on='token',how='outer')\n",
        "merged = merged.fillna(0)\n",
        "cols = ['noun_cluster_frequency','noun_cluster_documents',\n",
        "        'textrank_frequency','textrank_documents',\n",
        "        'yake_frequency','yake_documents',\n",
        "        'scake_frequency','scake_documents',\n",
        "        'sgrank_frequency','sgrank_documents',\n",
        "        'ngram_frequency','ngram_documents'\n",
        "        ]\n",
        "merged[cols] = merged[cols].astype(int)\n",
        "\n",
        "# Add word count\n",
        "merged['words'] = merged.apply(lambda x: len(count_words.findall(x['token'])), axis=1)\n",
        "\n",
        "merged.to_csv(output_filename.value,index=False)\n",
        "files.download(output_filename.value) \n",
        "\n",
        "merged"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y4u6yLZeOtL4"
      },
      "source": [
        "# [An explanation of PyTextRank: the algorithm](https://derwen.ai/docs/ptr/explain_algo/)"
      ]
    }
  ]
}