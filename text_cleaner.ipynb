{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "text_cleaner.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Arteric-Jeff-Knight/collabs/blob/master/text_cleaner.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S_AezM4YZE1s",
        "cellView": "form"
      },
      "source": [
        "#@title First, run this code block to load libraries and define functions.  Ignore any output that isn't an error.\n",
        "\n",
        "# First the easy stuff.\n",
        "from google.colab import files\n",
        "import ipywidgets as widgets\n",
        "import io\n",
        "import re, string, unicodedata                          # Import Regex, string and unicodedata.\n",
        "import numpy as np\n",
        "import pandas as pd                                     # Import pandas.\n",
        "import spacy\n",
        "\n",
        "# Then the stuff that has to be compiled\n",
        "\n",
        "!pip install emoji\n",
        "import emoji\n",
        "!pip install contractions\n",
        "import contractions                                     # Import contractions library.\n",
        "!pip install num2words\n",
        "from num2words import num2words\n",
        "\n",
        "# We are only using spacy to lemmatize the content and to get stop words\n",
        "spacy.cli.download('en_core_web_sm')\n",
        "nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
        "nlp.Defaults.stop_words -= {\"none\", \"nor\", \"not\", \"no\", \"one\"}\n",
        "\n",
        "# This regex will eliminate all stop words without tokenizing \n",
        "sw_pattern = re.compile(r'\\b(' + r'|'.join(nlp.Defaults.stop_words) + r')\\b\\s*')\n",
        "\n",
        "# Define the normalization funciton\n",
        "def normalize(content: str, remove_usernames: bool = True) -> str:\n",
        "  # Convert emojis\n",
        "  content = emoji.demojize(content)\n",
        "  # Remove non-ASCII\n",
        "  content = unicodedata.normalize('NFKD', content).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
        "  # Expand contractions\n",
        "  content = contractions.fix(content)\n",
        "  # remove URLs\n",
        "  content = re.sub(r\"http\\S+\", \"\", content)\n",
        "  # Remove leading RT_\n",
        "  content = re.sub(\"^[Rr][Tt] \",\"\",content)  \n",
        "  # Remove leading @somename\n",
        "  content = re.sub(\"^@[^ ]*\",\"\",content)\n",
        "  # If requested, remove all usernames\n",
        "  if remove_usernames:\n",
        "    content = re.sub(\"@[^ ]*\",\"\",content)\n",
        "  # Expand some symbols @todo others besides percent? \n",
        "  content = content.replace(\"%\",\" percent\")\n",
        "  content = content.replace(\" w/ \",\" with \")\n",
        "  # Replace all the ordinals\n",
        "  ordinals = re.findall(\"\\d+(?:st|nd|rd|th)\",content) # find all in string\n",
        "  if ordinals:\n",
        "    ordinals = set(ordinals) # reduce to unique values\n",
        "    for ordinal in ordinals:\n",
        "      content = content.replace(ordinal,num2words(ordinal[:-2],to=\"ordinal\") + ' ')\n",
        "  # Convert to lower case, then get rid of punctuation\n",
        "  content = re.sub(\"[^a-z0-9\\-\\/@ ]\",\"\",content.lower())\n",
        "  # Eliminate Stopwords\n",
        "  content = sw_pattern.sub('', content)\n",
        "  # Reduce multiple spaces\n",
        "  content = re.sub(\"\\s\\s+\", \" \", content).strip()\n",
        "  words = []\n",
        "  # Consider just splitting on the space to KISS\n",
        "  for word in content.split(' '):\n",
        "    try:\n",
        "      if word.isnumeric():\n",
        "        if int(word) < 999:\n",
        "          word = num2words(word)\n",
        "    except Exception as e:\n",
        "      pass\n",
        "    # put everything back together\n",
        "    words.append(word)\n",
        "  return ' '.join(words)\n",
        "\n",
        "\n",
        "def lemmatize(sentence: str):\n",
        "  doc = nlp(sentence)\n",
        "  return \" \".join([token.lemma_ for token in doc])\n",
        "\n",
        "def split_df_into_data_and_configs(uploaded, defaults: dict = {}, config_name: str = 'config'):\n",
        "  # Put uploaded file into dataframe\n",
        "  filename = list(uploaded.keys())[0]\n",
        "  df = pd.read_csv(io.BytesIO(uploaded[filename]),header=None)\n",
        "\n",
        "  # Get everything with 'configs' in first coumn\n",
        "  configs = df[df[0] == config_name] \n",
        "  # Build a dictionary from the key in the second column with values from the third\n",
        "  defaults.update(dict(zip(configs[1], configs[2])))\n",
        "\n",
        "  # Everything else that isn't a config, is data\n",
        "  data = df[df[0] != config_name].reset_index(drop=True)\n",
        "  # Assume that the first row is the column names now that configs are gone\n",
        "  data.columns = data.iloc[0]\n",
        "  # Drop the row with the column names\n",
        "  data.drop(df.index[0], inplace=True)\n",
        "  # Reset the index, so zero works below\n",
        "  data = data.reset_index(drop=True)\n",
        "\n",
        "  # Validate the column name configs\n",
        "  col_list = list(data.columns)\n",
        "\n",
        "  if 'col_in' not in defaults or defaults['col_in'] not in col_list:\n",
        "    # With nothing defined or garbage, use first column\n",
        "    if defaults['col_in'].capitalize() in col_list:\n",
        "      defaults['col_in'] = defaults['col_in'].capitalize()\n",
        "    else:\n",
        "      defaults['col_in'] = col_list[0]\n",
        "\n",
        "  if 'col_out' not in defaults:\n",
        "    defaults['col_out'] = 'clean_text'\n",
        "\n",
        "  if 'col_lem' not in defaults:\n",
        "    defaults['col_lem'] = 'lemmatized'\n",
        "\n",
        "  if 'file_out' not in defaults:\n",
        "    defaults['file_out'] = '-cleaned'\n",
        "\n",
        "  if 'drop_dupes' not in defaults:\n",
        "    defaults['drop_dupes'] = True\n",
        "  \n",
        "  if not isinstance(defaults['drop_dupes'], bool) and defaults['drop_dupes'].lower() in ['false','0',0]:\n",
        "    defaults['drop_dupes'] = False\n",
        "\n",
        "  if 'output_filename' not in defaults\n",
        "    defaults['output_filename'] = filename.replace('.csv',f\"{defaults['file_out']}.csv\")\n",
        "\n",
        "  return data, defaults"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tmBiXSLf1orU"
      },
      "source": [
        "# Next, run the following code block to upload a file for processing\n",
        "\n",
        "If you want to process multiple files, start here for each one, no need to run the first block again and again.\n",
        "\n",
        "Processing is mostly handled by default values, but if you need to override them, configuration is handled by passing values in the input file. See **How to Configure** below for instructions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sQZKIC06cJTz",
        "cellView": "form"
      },
      "source": [
        "#@title\n",
        "defaults = {\n",
        "  'col_in': 'content',\n",
        "  'col_out': 'clean_text',\n",
        "  'col_lem': 'lemmatized',\n",
        "  'file_out': '-cleaned',\n",
        "  'drop_dupes': True\n",
        "}\n",
        "\n",
        "df, configs = split_df_into_data_and_configs(files.upload(), defaults)\n",
        "\n",
        "print('---------- configs ----------')\n",
        "for key in configs:\n",
        "  print('   ',key,':',configs[key])\n",
        "print('-----------------------------')\n",
        "display(df)\n",
        "\n",
        "# Remove duplicate rows\n",
        "num_rows = df.shape[0]\n",
        "if configs['drop_dupes']:\n",
        "  df.drop_duplicates(configs['col_in'], inplace=True)\n",
        "  print(f'Processing {df.shape[0]} rows after deleting {num_rows - df.shape[0]} duplicates:\\n')\n",
        "else:\n",
        "  print(f'Processing {df.shape[0]} rows:\\n')\n",
        "\n",
        "# Apply the normalization function to the input file\n",
        "df[configs['col_out']] = df.apply(lambda x: normalize(x[configs['col_in']]), axis=1)\n",
        "df[configs['col_lem']] = df.apply(lambda x: lemmatize(x[configs['col_out']]), axis=1)\n",
        "\n",
        "print('Finished Processing\\n')\n",
        "\n",
        "print(f\"Saving locally to {defaults['output_filename']}\\n\")\n",
        "#This code downloads the result to your local machine.\n",
        "df.to_csv(defaults['output_filename'],index=False)\n",
        "files.download(defaults['output_filename'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uJ9s87UbG-fo"
      },
      "source": [
        "# Optional: for debugging, list 10 random results\n",
        "Run again and again to view different random rows"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yddOzvvMG-7j"
      },
      "source": [
        "#@title\n",
        "for idx in np.random.choice(df.shape[0], replace = True, size = 5):\n",
        "    print('----------')\n",
        "    print(df.loc[idx][configs['col_in']])\n",
        "    print('-')\n",
        "    print(df.loc[idx][configs['col_out']])\n",
        "    print('-')\n",
        "    print(df.loc[idx][configs['col_lem']])\n",
        "    print()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4LrljB3OkP44"
      },
      "source": [
        "\n",
        "# About the Normalization function\n",
        "\n",
        "### Here we define the steps to normalize the text:\n",
        "\n",
        "- Convert Unicode to ASCII and then back again\n",
        "    - This removes all emojis and accents and other garbage\n",
        "    - Convert back to unicode because later operations expect it\n",
        "- Expand Contractions\n",
        "    - For consistent grammar, expand it's to it is, etc.\n",
        "    - Also, future removal of punctation would change contractions to nonsense\n",
        "- Remove URLs\n",
        "    - They are not words\n",
        "- Remove 'RT ' from the start\n",
        "    - Many tweets begin with \"RT \" for retweet\n",
        "- Remove '@name: ' from start\n",
        "    - Even tweets that aren't retweets begin with \"@somename: \" which is garbage\n",
        "- Convert to lower case and remove punctuation\n",
        "    - For consistency \n",
        "- Change all spaces to single space and remove all leading and trialing spaces\n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E_3zpQL77Cft"
      },
      "source": [
        "\n",
        "# **How to Configure**\n",
        "\n",
        "To add configurations to a file, put the value 'config' in the first column (no matter what the header) and the configuration key name in the second, and the value to be set in the third (anything beyond that can be ignored). Durding processing these rows will be separated from the data and not included in the returned file. For convenience, they can appear anywhere in the incoming file: before the headers, at the end, anywhere in between or even mixed among the data.\n",
        "\n",
        "The possible configuration keys and their default values are:\n",
        "\n",
        "| Key          | Defualt    | Notes |\n",
        "|--------------|:-----------|:------|\n",
        "| `col_in`     | Content    | *The exact name of column in the incoming file to process* |\n",
        "| `col_out`    | clean_text | *The name of column to add the processed content* |\n",
        "| `col_lem`    | lemmatized | *The name of column to add the lemmatized processed content* |\n",
        "| `file_out`   | -cleaned   | *The text to add to the filename that is returned* |\n",
        "| `drop_dupes` | True       | *If this is set to True, then rows that duplicate content are dropped* |\n",
        "\n",
        "\n",
        "\n",
        "#### Coming Soon: Use configs to tweak normalization"
      ]
    }
  ]
}